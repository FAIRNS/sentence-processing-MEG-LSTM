\section{Materials and Methods}

\subsection{NA-tasks}

In all of our experiments, we use \emph{number-agreement (NA) tasks}.
\dieuwke{I started to explain NA-tasks, but I got a bit stuck on the fact that the task is actually quite different for humans and NLMs. Also, many of these things are actually already explained before, in the explanation of our earlier results. Wht do you suggest I do with this?}

For our two types of experiments, we use two six different NA-tasks, which we explain below.
An overview can be found in Table~\ref{tab:na-tasks-overview}, the full vocabulaire we used to generate our data in Appendix~\ref{appendix:lexicon}.

\subsubsection{Ablation experiments}
For our ablation studies, which we conduct to replicate and extend the findings of \citet{lakretz2019emergence} to italian, we use two different types of NA tasks.

\paragraph{NounPP}
First, to establish the presence of long-range \emph{number} units, we create an atialian version of \citeauthor{lakretz2019emergence}'s \citeyear{lakretz2019emergence} NounPP task.
This data set consists of 4000 sentences in which the subject is separated from the main verb by a prepositional phrase.
I.e., an example of a sentence in this dataset, which we dub \emph{nounPP}, is ``il \textbf{ragazzo} accanto alla \underline{donna} \textbf{conosce} \ldots'' (the \textbf{boy} next to the \underline{woman} \textbf{knows}).
We systematically vary the number of both the subject and the noun in the prepositional phrase (also referred to with the term \emph{attractor}), resulting in four different conditions: singular-singular (SS), singular-plural (SP), plural-singular (PS) and plural-plural (PP).

\paragraph{NounPP-gender}
To investigate if the model has long-range \emph{gender} units, we adapt the NounPP task to end with a predicative adjective.
For example: ``il \textbf{ragazzo} accanto alla \underline{donna} \`{e} \textbf{basso} (the \textbf{boy} next to the \underline{woman} is \textbf{short}).
In Italian, the gender of the adjective should match both the number and the gender of the subject.
We again consider four different conditions: feminine-feminine (FF), feminine-masculine (FM) masculine-feminine (MF) and masculine-masculine (MM), within which the number of the subject and attractor is ranomly varied.
The 4000 sentences in the NounPP-gender task are equally divided over these conditions.

\subsubsection{Nested experiments}

\dieuwke{April 30 -- I'll continue with this tomorrow}


\begin{table}[h!]
    \setlength\tabcolsep{2mm}
\small
\centering
\begin{tabular}{lll}
\multicolumn{3}{c}{\centering \textit{Probing number/gender units}}\\
\hline
\hline
\emph{Nounpp} & \texttt{\textbf{NP$_a$} prep NP$_b$ \emph{V$_a$}} & \specialcell{Il \textbf{ragazzo} accanto alla \underline{donna} \textbf{conosce}\vspace{-3mm}\\({\scriptsize The \textbf{boy} next to the \underline{woman} \emph{knows}})} \\
\emph{Nounpp-gender} & \texttt{\textbf{NP$_a$} prep NP$_b$ BE$_a$ \emph{ADJ$_a$}} & \specialcell{Il \textbf{ragazzo} accanto alla \underline{donna} \`{e} \textbf{basso}\vspace{-3mm}\\({\scriptsize The \textbf{boy} next to the \underline{woman} is \textbf{short}})}\\
~\\
\multicolumn{3}{c}{\centering \textit{Nesting experiments}}\\
\hline
\hline
\emph{Short-Successive} & \texttt{NP$_a$ V$_a$ che NP$_b$ V$_b$} & \specialcell{Il \textbf{figlio} \textbf{dice} che il \emph{ragazzo} \emph{ama}\vspace{-3mm}\\{\scriptsize The \textbf{son} \textbf{says} that the \emph{boy} \emph{loves}}} \\
\emph{Long-Successive} & \texttt{NP$_a$ V$_a$ che NP$_b$ P NP$_c$ V$_b$} & \specialcell{Il \textbf{figlio dice} che l'\emph{amico} accanto al \underline{ragazzo} \emph{conosce}\vspace{-3mm}\\{\scriptsize The \textbf{son says} that the \emph{friend} next to the \underline{boy} \emph{knows}}} \\
\emph{Short-nested} & \texttt{NP$_a$ che NP$_b$ V$_b$ V$_a$ } & \specialcell{Il \textbf{figlio} che il \emph{ragazzo} \emph{osserva} \textbf{evita}\vspace{-3mm}\\{\scriptsize The \textbf{son} that the \emph{boy} \emph{observes} \textbf{avoids}}} \\
\emph{Long-nested} & \texttt{NP$_a$ che NP$_b$ P NP$_c$ V$_b$ V$_a$} & \specialcell{Il \textbf{figlio} che la \emph{ragazza} accanto ai \underline{padri} \emph{ama} \textbf{evita}\vspace{-3mm}\\{\scriptsize The \textbf{son} that the \emph{girl} next to the \underline{fathers} \emph{loves} \textbf{avoids}}} \\
\end{tabular}
\caption{\textbf{The NA-tasks we use for our ablation and nesting experiments.}
The first column denotes the name of the task.
The second column indicates the format of the sentences, where \texttt{NP} is used as an abbreviation of \texttt{Det N}.
The indices $a$, $b$ help identify the noun-verb relationships in the templates.
I.e. in the \emph{long-nested} condition condition, there are three nouns and two verbs, the indices $a$ and $b$ indicate that the last verb \texttt{V$_a$} is syntactically dependent on the first noun phrase \texttt{NP$_a$}, whereas the penultimate verb \texttt{V$_b$} instead should match the features of the second noun phrase \texttt{NP$_b$}.
Note that for the long- and short-nested conditions, we test both the \emph{inner} verb \texttt{V$_b$} and the \emph{outer} verb \texttt{V$_a$}.
The third and last column contains an example of a sentence in the NA-task, along with its English translation.
Bold and italic face show the relations indicated by the indices in the templates.
For every NA-task, we systematically vary the \emph{number} (and in the gender-case also gender) of all nouns in the template, resulting in four different conditions (SS, SP, PS and PP) for the NA-tasks with two nouns (\emph{Nounpp}, \emph{Nounpp-gender}, \emph{Short-successive} and \emph{Short-nested}) and eight different conditions (SSS, SSP, SPS, SPP, PSS, PSP, PPS and PPP) for the NA-tasks with three nouns (\emph{Long-succesive} and \emph{Long-nested}).
The examples shown are all SS and SSS conditions.
\textbf{M: I agree that it might be a good idea to use NP, however this should also apply to the gender case \dieuwke{D: done}. Also, indexing is nice, but it should be used thoroughly \dieuwke{better like this?}. Also, it should also be shown in the examples. On the other hand, I am not sure of why only one noun/verb is capitalized/italicized for short- and long-nested, given that we test both main and embedded (perhaps, worth duplicating these cases?). \dieuwke{better like this?}}
}
\end{table}

\subsection{Behavioral Experiment}
\subsubsection{Participants}
61 psychology students from the University of Milano-Bicocca (males = XX; Age = XX ± XX; Education = XX ± XX) took part in the experiment in exchange of course credits. Participants were Italian native speakers and were naive with respect to the experiment purpose. The study was approved by the ethical committee of the Department of Psychology, and ethical treatment was in accordance with the principles stated in the Declaration of Helsinki.

\subsubsection{Stimuli}
Stimuli comprised i) acceptable sentences; ii) violation trials, containing a number violation on the verb of one of the subject-verb agreements; iii) filler sentences, comprising several syntactic and semantic violations. 

Acceptable sentences were created using a pool of 10 nouns, 19 verbs, 4 prepositions (see table S1).
Starting from this pool of sentences, number-violation and filler trials were created by replacing either the main or embedded verb by the opposite form of the verb with respect to number. For example, ``il fratello che lo studente *accolgono ama i contadini'' (``the brother that the student *welcome loves the farmers'').

Filler trials contained either semantic violations or a syntactic violation that does not concern number. Syntactic violations were generated by either i) replacing a verb with a wrong person without changing number, for example: ``il fratello che lo studente *accolgo ama i contadini'' (``the brother that the student *welcome-1st-pers-sing loves the farmers''); or ii) replacing a verb with a noun, for example, ``il fratello che lo studente *amica ama i contadini'' (``the brother that the student *friend loves the farmers''; note that the chosen replacement nouns were not ambiguous with verb forms in Italian); or iii) replacing a verb with its infinitive form, for example, ``il fratello che lo studente *accogliere ama i contadini'' (``the brother that the student *to-welcome loves the farmers''). Semantic violations were generated by either replacing one of the nouns with i) an inappropriate abstract one, for example, ``la *filosofia dice che la figlia ama la madre'' (``*philosophy says that the daughter loves the mother''); ii) or an inanimate noun, for example, ``la *matita dice che la figlia ama la madre'' (``the *pencil says that the daughter loves the mother''). To avoid correlation between abstract or inanimate nouns and semantic violations, half of these filler trials were felicitous, for example, ``il padre dice che la figlia ama la *filosofia'' (``the father says that the daughter loves *philosophy''), or ``il padre dice che la *matita appartiene alla figlia'' (``the father says that the *pencil belongs to the daughter''). See Supplementary Materials for more details.

In total, 540 sentences were presented to each participant, randomly sampled from a larger pool. Of these, 180 sentences were acceptable, 180 had a number violation, and 180 were fillers.

\subsubsection{Paradigm}
The experiment was conducted in two sessions of 270 trials each, which were performed by participants in different days. Each session lasted around 45 minutes. The two sessions took place at the same time of the day at a maximum temporal distance of two weeks. After receiving information about the experimental procedure, participants were asked to sign a written informed consent. 

Stimuli were presented on a 17” computer screen in a light-grey, 30-point Courier New font on a dark background. Sentences were presented using Rapid Serial Visual Presentation (RSVP). Each trial started with a fixation cross appearing at the center of the screen for 600 ms, then single words were presented with SOA=500 ms, 250 ms presentation followed by 250 ms of black screen. At the end of each sentence, a blank screen was presented for 1500 ms, then a response panel appeared, with two labels “correct” and “incorrect”, on two sides of the screen (in random order each time) for a maximal duration of 1500 ms. A final screen, showing accuracy feedback was presented for 500 ms.

Participants were informed that they would be presented with a list of sentences which could be acceptable or containing a syntactic or semantic violation. They were instructed to press the “M” key of the Italian keyboard as fast as possible once they detected a violation. Sentences were presented up to their end even when participants pressed the button earlier. Then, in the response panel, participants were asked to press the “X” key if the sentence was correct, or “M” when the sentence was not correct. During the entire session, participants were asked to keep their left index over “X” and their right index over “M”. After each trial, participants received feedback concerning their response: ``Bravo!'' (``Good!'') in  case the sentence was correct, ``Peccato..'' (i.e. ``too bad...'') when it was incorrect. \textbf{M: Rather than the sentence, the response?} At the beginning of each session, participants performed a training block comprising XX items. The training section included all types of stimuli.

\subsubsection{Data and Statistical Analyses}
In ungrammatical trials, a violation could occur on either the main or embedded verb. Errors therefore correspond to trials in which a violation was missed. Since in ungrammatical trials a violation occurred on only one of the two verbs, the error can be associated with either the main or embedded dependency. In grammatical trials, errors correspond to trials in which participants reported a violation despite its absence. In contrast to ungrammatical trials, in which the violation marks the dependency, in grammatical trials it is not possible to associate an error with one of the two dependencies. Moreover, due to the presence of filler trials, the false detection of a violation could be unrelated to grammatical agreement (for example, a false detection of a semantic violation). Agreement errors were therefore estimated from ungrammatical trials only.

Statistical analyses were carried out using R, an open-source programming language \citep{R}. For each hypothesis to be tested, we fitted a mixed-effects logistic regression model \citep{Jaeger2008}, with participant and item as random factors, using the lme4 package for linear mixed effects models \citep{Bates}. Following \citet{Baayen:etal:2008}, we report the results from the model with the maximal random-effects structure that converged for all experiments. 
%Data from participants with accuracy below 0.8 on the filler items were excluded from analyses.

\subsection{Language Model}
\subsubsection{Model Description}
We use the Italian NLM made available by \citet{Gulordava:etal:2018} at \url{https://github.com/facebookresearch/colorlessgreenRNNs}.
It consists of two layers with 650 Long-Short Term Memory (LSTM) units \citep{Hochreiter:Schmidhuber:1997}, input and output embedding layers of 650 units and input and output layers of size 50000 (the size of the vocabulary). The weights of the input and output embedding layers are not shared.
The last layer of the model is a softmax layer, whose activations sum up to 1 and as such corresponds to a probability distribution over all words in the NLM's vocabulary. The model was trained on a dump of the Italian Wikipedia (80M word token, 50K word types). See  \citet{Gulordava:etal:2018} for further details.

\subsubsection{Control model Training} 
\textbf{M: Given how little emphasis this has in the main text, how about moving it to supplementary?} In addition to the NLM made available by \citet{Gulordava:etal:2018}, we train an additional 19 models using the same procedure and corpus (drawn from Wikipedia), giving 20 models in total. 
The models differ in the order in which those sentences are presented as well as the initialization of their weights.
For all runs, we use a learning rate of 20, a batch size of 64 and a dropout rate of 0.2, the hyperparameters that \citet{Gulordava:etal:2018} reported to work best for this particular corpus and setup.
Following Gulordava, but contrary to common practice in language modeling, we train the models on separate sentences, rather than longer pieces of discourse.
As common practice for training language models, we do not use an optimizer, but instead use a \emph{plateau-based} learning scheme, in which we half the learning rate whenever the validation perplexity of the model reaches a plateau.

\subsubsection{Model Evaluation} After training, we evaluate the resulting 20 models by considering their perplexity on a shared test set\footnote{\url{https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/Italian/test.txt}}. For the NA-tasks, following \citet{Linzen:etal:2016}, we compute accuracy by presenting the preamble of each sentence to the NLM and then compare the output probabilities assigned to the plural and singular forms of the verb.\textbf{How about the gender experiment?} On each sentence, the model is scored 1 if the probability of the correct verb is higher than the wrong, and else 0. Model accuracy is then defined as the average of these scores across all sentences in the NA-task. \textbf{M: I think it would be better to only refer here to evaluation of the main model.}

\subsubsection{Ablation Experiments}
To identify units that play an important role in the encoding of number and gender, we run a series of ablation tests.
In these ablation tests, we assess the impact of a \emph{single unit} on model performance by setting the activation of the unit to zero and then recomputing the performance of the model on the Noun-PP NA-task. \textbf{M: consistency in how the tasks are called: Noun-PP, Nounpp, NounPP\ldots} 
We conduct such ablation studies for all recurrent units in the network, resulting in 1300 ablation studies per model.

