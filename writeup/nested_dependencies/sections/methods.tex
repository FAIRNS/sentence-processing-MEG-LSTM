\section{Materials and Methods}

\subsection{NA-tasks}

\begin{table}
    \setlength\tabcolsep{2mm}
\small
\centering
\begin{tabular}{lll}
\multicolumn{3}{c}{\centering \textit{Finding number/gender units}}\\
\hline
\hline
\emph{Nounpp} & \texttt{\textbf{NP$_1$} prep NP$_2$ \emph{V}} & \specialcell{Il \textbf{ragazzo} accanto alla donna \emph{conosce}\vspace{-3mm}\\({\scriptsize The \textbf{boy} next to the woman \emph{knows}})} \\
   \emph{Nounpp-gender} & \texttt{det1 \textbf{N1} prep det2 N2 V \emph{ADJ}} & \specialcell{Il \textbf{ragazzo} accanto alla donna \'{e} \emph{basso}\vspace{-3mm}\\({\scriptsize \textbf{boy} next to the woman is \emph{short}})}\\
~\\
\multicolumn{3}{c}{\centering \textit{Nestedness experiments}}\\
\hline
\hline
\emph{Short-Successive} & \texttt{NP$_a$ V$_a$ che NP$_b$ V$_b$} & \specialcell{Il figlio dice che il \textbf{ragazzo} \emph{ama}\vspace{-3mm}\\{\scriptsize The son says that the \textbf{boy} \emph{loves}}} \\
\emph{Long-Successive} & \texttt{NP$_a$ V$_a$ che NP$_b$ P NP$_c$ V$_b$} & \specialcell{Il figlio dice che l'\textbf{amico} accanto al ragazzo \emph{conosce}\vspace{-3mm}\\{\scriptsize The son says that the \textbf{friend} next to the boy \emph{knows}}} \\
\emph{Short-nested} & \texttt{NP$_a$ che NP$_b$ V$_b$ V$_a$ } & \specialcell{Il \textbf{figlio} che il ragazzo osserva \emph{evita}\vspace{-3mm}\\{\scriptsize The \textbf{son} that the boy observes \emph{avoids}}} \\
\emph{Long-nested} & \texttt{NP$_a$ che NP$_b$ P NP$_c$ V$_b$ V$_a$} & \specialcell{Il \textbf{figlio} che la ragazza accanto ai padri ama \emph{evita}\vspace{-3mm}\\{\scriptsize The \textbf{son} that the girl next to the fathers loves \emph{avoids}}} \\
\end{tabular}
\caption{NA-tasks, \dieuwke{Add explanation. To save space, I used \texttt{NP} as an abbreviation for \texttt{Det N}. While this is of course an NP, it is also ab it confusing.}}
\end{table}

\subsection{Behavioral Experiment}
\subsubsection{Participants}
61 psychology students from the University of Milano-Bicocca (males = XX; Age = XX ± XX; Education = XX ± XX) took part in the experiment in exchange of course credits. Participants were Italian native speakers and were naive with respect to the experiment purpose. The study was approved by the ethical committee of the Department of Psychology, and ethical treatment was in accordance with the principles stated in the Declaration of Helsinki.

\subsubsection{Stimuli}
Stimuli comprised i) acceptable sentences; ii) violation trials, containing a number violation on the verb of one of the subject-verb agreements; iii) filler sentences, comprising several syntactic and semantic violations. 

Acceptable sentences were created using a pool of 10 nouns, 19 verbs, 4 prepositions (see table S1).
Starting from this pool of sentences, number-violation and filler trials were created by replacing either the main or embedded verb by the opposite form of the verb with respect to number. For example, ``il fratello che lo studente *accolgono ama i contadini'' (``the brother that the student *welcome loves the farmers'').

Filler trials contained either semantic violation or a syntactic violation that is not with respect for number. Syntactic violations were generated by either i) replacing to a verb in a wrong person, for example, ``il fratello che lo studente *accolgo ama i contadini''; ii) replacing a verb with a noun, for example, ``il fratello che lo studente *amica ama i contadini'' (the brother that the student *friend loves the farmers); iii) replacing a verb with its infinitive form, for example, ``il fratello che lo studente *accogliere ama i contadini'' (``the brother that the student *to welcome loves the farmers''). Semantic violations were generated by either replacing one of the noun with i) an inappropriate abstract one, for example, ``la *filosofia dice che la figlia ama la madre'' (the ``*philosophy says that the daughter loves the mother''); ii) inanimate noun, for example, ``la *matita dice che la figlia ama la madre'' (``the *pencil says that the daughter loves the mother''). To avoid correlation between abstract or inanimiate nouns and semantic violation, half of these filler trials were felicitous, for example, ``il padre dice che la figlia ama la *filosofia'' (``the father says that the daughter loves the *philosophy'', or ``il padre dice che la *matita appartiene alla figlia'' (``the father says that the *pencil belongs to the daughter''). See Supplementary Materials for more details.

In total, 540 sentences were presented to each participant, which were randomly sampled from a larger pool of sentences. From these, 180 sentences were acceptable, 180 with number violation, and 180 fillers. 

\subsubsection{Paradigm}
The experiment was conducted in two sessions (270 trials in each), which were performed by participants in different days. Each session lasted around 45 minutes. The two sessions took place at the same time of the day at a maximum temporal distance of two weeks. After receiving information about the experimental procedure, participants were asked to sign a written informed consent. 

Stimuli were presented on a 17” computer screen in a light-grey, 30-point Courier New font on a dark background. Sentences were presented using a Rapid Serial Visual Presentation (RSVP). Each trial started with a fixation cross appearing at the center of the screen for 600 ms, then single words were presented with SOA=500 ms, 250 ms presentation followed by 250 ms of black screen. At the end of each sentence, a blank screen was presented for 1500 ms, then a response panel appeared, with two labels “correct” and “incorrect”, on two sides of the screen (in random order each time) for a maximal duration of 1500 ms. A final screen, showing accuracy feedback was presented for 500 ms.

Participants were informed that they would be presented with a list of sentences which could be acceptable or containing a syntactic or semantic violation. They were instructed to press the “M” key of the Italian keyboard as fast as possible once they detected a violation. Sentences were presented up to their end even when participants pressed the button earlier. Then, in the response panel, participants were asked to press the “X” key if the sentence was correct, or “M” whether the sentence was not correct. During the entire session, participants were asked to keep their left index over “X” and their right index over “M” keys. After each trial participants received a feedback concerning their response: ``Bravo!'' (i.e. ``Good!'') in the case that sentence was correct, ``Peccato..'' (i.e. ``too bad...'') when it was incorrect. At the beginning of each session, participants performed a training block comprising XX items. The training section included all types of stimuli.

\subsubsection{Data and Statistical Analyses}
In ungrammatical trials, a violation could occur on either the main or embedded verb. Errors therefore correspond to trials in which a violation was missed. Since in ungrammatical trials a violation occurred on only one of the two verbs, the error can be associated with either the main or embedded dependency. In grammatical trials, errors correspond to trials in which participants reported a violation despite its absence. In contrast to ungrammatical trials, in which the violation marks the dependency, in grammatical trials it is not possible to associate an error with one of the two dependencies. Moreover, due to the presence of filler trials, the false detection of a violation could be unrelated to grammatical agreement (for example, a false detection of a semantic violation). Agreement errors were therefore estimated from ungrammatical trials only.

Statistical analyses were carried out using R, an open-source programming language \citep{R}. For each hypothesis testing, we fitted a mixed-effects logistic regression model \citep{Jaeger2008}, with participant and item as random factors, using the lme4 package for linear mixed effects models \citep{Bates}. Following \citet{Baayen:etal:2008}, we report the results from the model with the maximal random effects structure that converged for all experiments. 
%Data from participants with accuracy below 0.8 on the filler items were excluded from analyses.

\subsection{Language Model}
\subsubsection{Model Description}
The model architecture we use is identical to the architecture presented by \citet{Gulordava:etal:2018}. 
It consists of two layers with 650 Long-Short Term Memory (LSTM) units \citep{Hochreiter:Schmidhuber:1997}, input and output embedding layers of 650 units and input and output layers of size 50000 (the size of the vocabulary). The weights of the input and output embedding layers are not shared.
The last layer of the model is a softmax layer, whose activations sum up to 1 and as such corresponds to a probability distribution over all words in the NLM's vocabulary.

\subsubsection{Model Training} 
In addition to the NLM made available by \citet{Gulordava:etal:2018}, we train an additional 19 models using the same procedure and corpus (drawn from Wikipedia), giving 20 models in total. 
The models differ in the order in which those sentences are presented as well as the initialisation of their weights.
For all runs, we use a learning rate of 20, a batch size of 64 and a dropout rate of 0.2, the hyperparameters that \citet{Gulordava:etal:2018} reported to work best for this particular corpus and setup.
Following Gulordava et. al, but contrary to common practice in language modelling, we train the models on separate sentences, rather than longer pieces of discourse.
As common practice for training language models, we do not use an optimiser, but instead use a \emph{plateau-based} learning scheme, in which we half the learning rate whenever the validation perplexity of the model reaches a plateau.

\subsubsection{Model Evaluation} After training, we evaluate the resulting 20 models by considering their perplexity on a shared test set\footnote{\url{https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/Italian/test.txt}}. For the NA-tasks, following \citet{Linzen:etal:2016}, we compute accuracy by presenting the preamble of each sentence to the NLM and then compare the output probabilities assigned to the plural and singular forms of the verb. On each sentence, the model is scored 1 if the probability of the correct verb is higher than the wrong, and else 0. Model accuracy is then defined as the average of these scores across all sentences in the NA-task.

\subsubsection{Ablation Experiments}
To identify units that play an important role in the encoding of number and gender, we run a series of ablation test.
In these ablation tests, we assess the impact of a \emph{single unit} on model performance by setting the activation of the unit to zero and then recomputing the performance of the model on the Noun-PP NA-task. 
We conduct such ablation studies for all recurrent units in the network, resulting in 1300 ablation studies per model.

