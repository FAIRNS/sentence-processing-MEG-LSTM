\abstract{Recursive processing in sentence comprehension is considered a hallmark of human linguistic abilities. However, its underlying neural mechanism in the human brain remains largely unknown. Neural Language Models (NLMs) have recently shown remarkable success on various linguistic tasks. As currenlty the only non-human computational system capable of accomplishing such tasks, NLMs suggest new opportunities to study neural mechanisms underlying natural language processing, which might give insights about similar processes in humans. We use subject-verb number agreement as an index of syntactic processing in NLMs and investigate the neural mechanisms underlying processing of nested agreements --- a prototypical case of recursive structures. We found that a neural agreement mechanism in the model can successfully handle outermost agreements of nested constructions, however, albeit shorter, the model makes more errors on embedded dependencies, and dramatically fails when the embedded dependency is long-range. We derive predictions about human performance based on these error patterns in NLMs and on properties of its agreement mechanism and test them in a behavioral experiment. Results show that unlike NLMs, humans do not exhibit a dramatic failure on long-range embedded dependencies. However, overall, error patterns of humans and NLMs are remarkably similar, suggesting NLMs as compelling models for sentence processing that can account for various effects in human data. To test whether the main discrepancy is quantitative (e.g., dependent on the number of nesting) or qualitative (e.g., due to recursive processing unique to humans), we conclude by listing testable predictions for future experiments.}