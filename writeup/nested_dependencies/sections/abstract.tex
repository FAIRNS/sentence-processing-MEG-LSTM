\abstract{Recursive processing in sentence comprehension is considered a hallmark of human linguistic abilities. However, its underlying neural mechanism in the human brain remains largely unknown. Neural Language Models (NLMs) have recently shown remarkable success on various linguistic tasks. As the only non-human computational systems capable of accomplishing such tasks, NLMs offer new opportunities to study low-level mechanisms underlying natural language processing, which might give insights about human language processing. Here, we study syntactic processing in an NLM based on Recurrent Neural Networks with Long-Short Term Memory units. We use subject-verb number agreement as an index of syntactic processing in the NLM, and investigate the neural mechanisms underlying processing of nested agreements--a prototypical case of recursion. We find a small set of specialized units in the model that can successfully handle outermost agreements of nested constructions. An analysis of this mechanism predicts that it will however run into trouble when processing embedded dependencies, especially if they are in turn long-range. We confirm this in simulations and further derive predictions about human performance based on the properties of the NLM agreement mechanism, and test them in a behavioral experiment. Results show that, unlike the NLM, humans do not exhibit a dramatic failure on embedded long-range dependencies. However, overall, the error patterns of humans and the NLM are remarkably similar, suggesting NLMs as compelling models for sentence processing that can account for various effects in human data. We conclude that the NLM explains a large variance in human data, however, the network fails to find recursive processing of nested constructions.}

%We conclude by outlining future experiments that should establish whether the main discrepancy is quantitative (e.g., dependent on the number of nesting) or qualitative (e.g., due to recursive capabilities unique to humans).}