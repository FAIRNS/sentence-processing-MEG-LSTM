\abstract{Recursive processing in sentence comprehension is considered a hallmark of human linguistic abilities. However, its underlying neural mechanism in the human brain remains largely unknown. Neural Language Models (NLMs) have recently shown remarkable success on various linguistic tasks. As the only non-human computational systems capable of accomplishing such tasks, NLMs offer new opportunities to study low-level mechanisms underlying natural language processing, which might give insights about human language processing. We use subject-verb number agreement as an index of syntactic processing in NLMs, and investigate the neural mechanisms underlying processing of nested agreements--a prototypical case of recursion. We find a neural circuit in the model that can successfully handle outermost agreements of nested constructions. An analysis of this mechanism predicts that it will however run into trouble when processing embedded dependencies, especially if they are in turn long-range. We confirm this prediction empirically. We further derive predictions about human performance based on the properties of the NLM agreement mechanism, and test them in a behavioral experiment. Results show that, unlike NLMs, humans do not exhibit a dramatic failure on embedded long-range dependencies. However, overall, the error patterns of humans and NLMs are remarkably similar, suggesting NLMs as compelling models for sentence processing that can account for various effects in human data. We conclude by outlining future experiments that should establish whether the main discrepancy is quantitative (e.g., dependent on the number of nesting) or qualitative (e.g., due to recursive capabilities unique to humans).}