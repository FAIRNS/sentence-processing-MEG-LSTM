\section{Introduction}

% \input{figures/fig_design.tex}

\begin{itemize}
\item Neural language models (NLMs) are wonderful blah blah blah, revived interest in
  looking at them as computational models of language processing
\item Current research in the area focuses on understanding the behaviour of NLMs wrt
  various grammatical phenomena; some work showing correlations
  between internal states and said phenomena
\item Number agreement is an area where some steps have been taken
  towards mechanistic understanding:
  \begin{itemize}
  \item sparse feature-propagation mechanism controlled by distributed
    grammar network
  \end{itemize}
\item Our goal here is two-fold:
  \begin{itemize}
  \item First, we replicate our earlier results on the emergence of
    this mechanism in a different language, as well as extending them to a different
    agreement phenomenon (\textbf{do we?}) confirming it is no fluke
  \item Second, we use our understanding of of how agreement is
    performed by NLMs to make a new prediction about a difficulty in agreement
    processing in humans (analogy to DiCarlo's work in vision?)
  \end{itemize}
\item Main result: the predicted pattern is confirmed; however, in other ways,
  human and NLM patterns differ, confirming that studying NLMs is
  productive, but the right perspective is not to treat them as
  full-fledged cognitive models, but rather as powerful computational
  systems that, when faced with challenges similar to those
  encountered by humans, might adopt partially analogous solutions
\end{itemize}
