\section{Number Agreement in Neural Language Models}
In the classic number agreement task (NA-task), subjects are presented with the beginning of a sentence (aka, `preamble') that contains a long-range subject-verb relation, such as: ``The keys to the cabinet\ldots'', and are asked to predict the verb to follow (e.g., ``are''). Human subjects make more agreement errors (e.g., continuing the preamble  above with ``is'' instead of ``are'') when the intervening noun (aka, `attractor') has a different grammatical number than the main subject (as in the preamble above, with plural subject ``keys'' and singular attractor ``cabinet'') . Behavioral measures collected during agreement tasks, such as error-rates, vary as a function of the syntactic environment of the long-range dependency. This has provided rich data to test hypotheses regarding online syntactic processing in humans \citep[e.g., ][]{franck2002subject, franck2006agreement, franck2007syntactic}.

Starting with the influential work of \citet{Linzen:etal:2016}, a growing
literature \citep[e.g.,][]{Gulordava:etal:2018, Bernardy:Lappin:2017,
  Giulianelli:etal:2018, Kuncoro:etal:2018a,Linzen:Leonard:2018,jumelet2019analysing} has
tested NLMs on the NA-task at the behavioural level, showing that these models have performance
and error patterns partially resembling those of humans. %
% In a seminal\dnote{Perhaps for this domain calling this work seminal is a bit much?} work,  showed that, like humans, recurrent NLMs can also solve the number-agreement task, and that the model seems to have error patterns that roughly resemble that of humans.
% This has since led to a proliferation of studies using agreement-tasks as a probe into syntactic processing in NLMs and evaluation of processing similarities between humans and NLMs \citep[e.g.,][]{Linzen:Leonard:2018}.
More recently, we investigated the underlying neural mechanism of an
English NLM during the processing of a long-range dependency
\citep{lakretz2019emergence}. We identified a neural circuit in the
network that encodes and carries grammatical number across long-range
dependencies, showing also that processing in NLMs is sensitive to the
structure of the subject-verb dependency. In the next section we
describe the main findings of our previous study.

\subsection{Processing of a Single Dependency in NLMs}
\subsubsection{The Noun-PP Number-Agreement Task}
The main NA-task task used by \citet{lakretz2019emergence} contained sentences with a subject-verb dependency separated by a prepositional phrase (e.g., ``The \textbf{boy} near the car \textbf{smiles}''), referred to as the `Noun-PP' task. This task comprises four conditions, which correspond to the four possible assignments of grammatical number to the main subject and attractor (SS, SP, PS and PP; S-singular, P-plural). The NLM was presented with preambles of sentences from this task, and predictions of the model of the next word were then extracted, from which error-rates were computed. 

\subsubsection{Long-Range Number Units}
Having verified that the network could predict the correct number with high accuracy, \citet{lakretz2019emergence} tested whether there were units in the network that were crucial to carry grammatical number across long-range dependencies.  To identify such units, we conducted an ablation study, by removing a unit of the NLM at a time, and re-evaluating its performance on the Noun-PP NA-task. \dnote{Can we actually say ``we'' here? Perhaps it is better to refer to ourselves in the third person?} The ablation revealed that two (out of 1300) units in the network each caused a reduction in agreement performance towards chance level when ablated. One of these units only affected performance when the main subject of the sentence was singular, and was therefore called the `singular unit'. The other unit had an effect with plural subjects only, hence, the `plural unit'. 
No other unit had a comparable effect on network performance when ablated. A visualization of state dynamics of the singular and plural units confirmed their role in encoding and carrying through grammatical number across long-range dependency, robustly also in the presence of an attractor \citep[figure 1 in][]{lakretz2019emergence}.

\subsubsection{Syntax Units}
Since the activity of the long-range number units follow the structure of the syntactic long-range dependency, we hypothesized that there should be other units in the network that encode syntactic information and inform the long-range number units about when to store and release number information in their encoding.  Using decoding methods, we found that there exist a set of other units in the network whose activity is predictive of transient syntactic properties of the sentence. 
In particular, we found that the activity of one of these units followed the structure of the main subject-verb dependency, consistently across various sentence constructions \citep[figure 3 in][]{lakretz2019emergence}. 
We refer to this unit as the `syntax unit'.

\subsubsection{The Long-Range Number-Agreement Circuit}
Finally, a network connectivity analysis revealed that the syntax unit indeed had exceptionally strong efferent connections to both long-range units, compared to all other units in the network. This was shown to control writing and erasing of grammatical number information in both long-range units. Taken together, the three units (singular, plural and syntax) form a neural circuit dedicated to long-range number-agreement dependency processing. Whenever a long-range dependency occurs, the syntax unit traces it by its activity and conveys this information to the long-range number units, which in turn store and carry the grammatical number of the subject up to the main verb. 

\subsubsection{Short-Range Number Units}
We further found that grammatical number information was also encoded
by other units in the network. Indeed, number information can still be
decoded from network activity even when the long-range number units
are removed. However, number information encoded in these other number
units is short-lived. Whenever new grammatical number information is
introduced (e.g., upon encountering a noun or a verb), activity in
these units abruptly switches to represent this last encountered
number. We therefore refer to them as `Short-Range Number Units',
since they can only support number-agreement dependencies that do not
enfold attractors (e.g., "The \textbf{boy} gracefully
\textbf{smiles}"). The presence of short-range number units explain
why ablating the long-range circuit only affects agreement in long-distance dependencies.

\subsection{Processing of Two Dependencies in NLMs}
Our earlier study revealed that the core mechanism for long-range dependencies is composed of a very small number of NLM units--two long-range number units and a syntax unit. So far we have analyzed the dynamics of an NLM during the processing of sentences with a single long-range dependency. We next ask: given that the long-range agreement mechanism is sparse, and can thus only encode a single number feature at a time, how will the NLM process \emph{two} long-range dependencies that are active at once?

Two simultaneously active long-range dependencies occur in various constructions, such as center-embedded nested dependencies, a prototypical example of recursion in language.. In nested dependencies, once the long-range agreement mechanism is engaged in tracking of the main dependency, there may be no more suitable units available to process the \textit{embedded} agreement. For example, in the sentence``The \textbf{boy} that the \textit{farmer} near the \underline{fathers} \textit{watches} \textbf{knows} the daughters'', there are two grammatical numbers that need to be carried across a long-range dependency: (1) that of the main subject `boy'', and (2) that of the embedded subject `farmer'. Once the NLM  encounters the main subject, its grammatical number can be stored through the long-range agreement mechanism up to the main verb `knows'. However, during this period, since the mechanism is already taken up, once the embedded subject `farmer' is presented to the network, there is no robust way to encode and carry its number up to the embedded verb `watches'. The NLM is thus predicted to fail to process the embedded dependency in nested structures.

We emphasize that the failure of the model is predicted under two conditions:
\begin{itemize}
	\item The two dependencies are active \textit{simultaneously} at some point: if this is not the case, i.e., the dependencies are successive (e.g., ``The \textbf{boy} near the cars \textbf{says} that the \textit{farmer} near the \underline{fathers} \textit{watches} the daughters''), the long-range mechanism can first complete the processing of the first dependency and reset before encoding the next one. 

        \item Both dependencies are \textit{long-range}: in the case
          of a short-range dependency nested within a long-range one
          (e.g., ``The \textbf{boy} that the \textit{farmer}
          \textit{watches} \textbf{knows} the daughters''), the
          embedded short-range dependency can still be processed by
          the short-range units we described above, although possibly
          in a less robust way.
\end{itemize}

\YL{[We should probably discuss at some point the fact that humans \textit{can} process two long-range dependencies in various cases (e.g., cross dependencies, a clause with sentential complement embedded inside an objrel (e.g, Gibson 98)].}
