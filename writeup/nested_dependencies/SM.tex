\documentclass[a4paper, 11pt]{article}

% Added by us
	
\usepackage{pdflscape}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{caption}
\usepackage{float}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{array}
\usepackage{color, colortbl}
\usepackage{arydshln}
\usepackage{xr}
\newcolumntype{P}[1]{>{\centering\arraybackslash}m{#1}}
% \renewrobustcmd{\bfseries}{\fontseries{b}\selectfont}
% \renewrobustcmd{\boldmath}{}
% \newrobustcmd{\B}{\bfseries}

\newcommand{\dieuwke}[1]{\textcolor{blue}{DH: #1}}
\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }

\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}

\begin{document}
\beginsupplement

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/SM/Ablation_results_K_model.png}
    \caption{\textbf{Ablation results for the model from Gulordave et. al, 2018}}
    \label{fig:ablation_K_model}
\end{figure}

\begin{landscape}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/SM/Ablation_results_all_models.png}
    \caption{\textbf{Ablation results for all 20 models}}
    \label{fig:ablation_all_models}
\end{figure}
\end{landscape}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/SM/FigureS2_output_weights.png}
    \caption{\textbf{\textbf{Efferent weights of number and gender units}. A: }}
    \label{fig:ablation_all_models}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=16cm]{figures/SM/error_rates_all_conditions.png}
    \caption{\textbf{Error rates for all conditions:} collected from the NLM (panels A \& B) and human subjects (C \& D). Bluish and reddish colors correspond to whether the main and embedded subjects agree on number (congruent subjects) or not (incongruent), respectively. Secondary colors (cyan or magenta) represent the presence of a nested attractor carrying an opposite grammatical number. Bars with stripes correspond to conditions in which the main subject is plural. Error bars represent standard error of the mean across all trials.}
    \label{fig:error_rates_all_conditions}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=16cm]{figures/SM/error_rates_acceptable_by_congruence.png}
    \caption{\textbf{Error rates on grammatical sentences:} for successive (panel A) and nested dependencies (B). Blue and red correspond to whether the main and embedded subjects agree on number (congruent subjects) or not (incongruent), respectively. Error bars represent the 95\% confidence level.}
    \label{fig:error_rates_all_conditions}
\end{figure}

% \input{tables/table_stats_number}

\input{tables/lexicon_table}

\end{document}

% \subsubsection{Control model Training} 
% \textbf{M: Given how little emphasis this has in the main text, how about moving it to supplementary? \dieuwke{Agree}} In addition to the NLM made available by \citet{Gulordava:etal:2018}, we train an additional 19 models using the same procedure and corpus (drawn from Wikipedia), giving 20 models in total. 
% The models differ in the order in which those sentences are presented as well as the initialization of their weights.
% For all runs, we use a learning rate of 20, a batch size of 64 and a dropout rate of 0.2, the hyperparameters that \citet{Gulordava:etal:2018} reported to work best for this particular corpus and setup.
% Following Gulordava, but contrary to common practice in language modeling, we train the models on separate sentences, rather than longer pieces of discourse.
% As common practice for training language models, we do not use an optimizer, but instead use a \emph{plateau-based} learning scheme, in which we half the learning rate whenever the validation perplexity of the model reaches a plateau.

% After training, we evaluate the resulting 20 models by considering their perplexity on a shared test set\footnote{\url{https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/Italian/test.txt}}. 

\subsection{Markedness effect in humans and NLM}
\textbf{M: What is the reason to put this in the main? The issue of whether plural is also a marked feature for NLMs is interesting (for example, to test theories that markedness is a function of statistical trends that should also be picked up by language models). However, in the context of our discussion, I find it quite distracting. I would vote for moving it entirely to the appendix (together with the full-results analyses above).}

Finally, we tested for markedness effects in both humans and the NLM.
For each verb in each of the four NA-task, we fitted a logistic-regression model with subject-congruence and grammatical number of the intervening subject as variables. Note that, while for embedded verbs the intervening subject is the main subject, for main verbs, the intervening subject is the embedded subject. In the case of Long-Successive and Long-Nested, we also included congruence of the embedded subject and the nested attractor as a variable in the regression model. 

In humans, we found significant main effects for the grammatical number of the intervening subject in the following cases: embedded and main verbs in Short- and Long-Nested (Short-Nested: $\beta^{embedded}=0.47; p-value<0.05$ and $\beta^{main}=0.56; p-value<0.05$; Long-Nested: $\beta^{embedded}=0.67; p-value<0.001$ and $\beta^{main}=0.47; p-value<0.05$; positive $\beta$ indicates more errors due to intervention of a plural number). In Long-Nested, we further found an interaction between subject-congruence and the grammatical number of the intervening attractor ($\beta^{interaction}=1.4173; p-value<0.001$). In contrast, in the NLM, an intervening plural noun did not generate more errors compared to singular. In several cases, the effect was reversed, i.e., an intervening singular noun caused more errors - Table S1\ref{} summarizes all effects found in both humans and the NLM. 
