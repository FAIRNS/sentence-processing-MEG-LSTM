\documentclass{article}

\title{Supplementary Material}
\date{}
\author{}

\begin{document}

\maketitle

\section*{Number Agreement data}

For our experiments we use several 7 synthetically generated datasets, that each contain sentences with one particular syntactic structure, and varied lexical material.
Each construction is stored in a distinct file.
Each file contains 4 columns, that are separated by tabs:\begin{enumerate}
    \item the first column contains the sentence
    \item the second column  contains the number of the subject (singular or plural)
    \item the third column contains whether the main verb of the sentence agrees with the subject (`correct') or not (`wrong'). Each of the sentences is present twice, once with the correct agreement and once with the wrong one.
    \item the fourth column contains the sentence id (which is the same for both correct and wrong version of the same sentence).
\end{enumerate}

The results of the paper can be reproduced using the scripts in our github repository, which will be made available upon acceptance.
% the sentences to reproduce the results of the paper, optionally with your own model. 
% If you want to use these sentences to reproduce the results of the paper, optionnaly with your own model, 
% you can pass each file to the script ../convert\_sentences\_to\_tal\_format.pl, which will create a .text and a .gold file, 
% that are directly read by the /sentence-processing-MEG-LSTM/Code/LSTM/model-analysis/ablation-experiment.py script. 

% The full ablation pipeline can be found in sentence-processing-MEG-LSTM/Code/LSTM/model-analysis/run\_full\_ablation\_pipeline.sh and analyse\_ablation\_results.sh

\section*{Tree Depth Data}

For our regression experiments we used a large corpus with sentences with unambiguous but varied syntactic structures, generated by a script that follows a predefined context free grammar.
The output of this script is a four column file containing:\begin{enumerate}
    \item The generated sentence.
    \item The syntactic parse tree of the sentence, according to the grammar that was used to generate it.
    \item The depth of the syntactic parse tree for each word position (i.e., corresponding to the number of open brackets)
    \item The number of open syntactic nodes at each word position (as described in Nelson et al., 2017)
\end{enumerate}

\noindent The columns of the file are separated with the character $|$.

We first processed a large corpus of such sentence with lengths between 2 and 25 words with our LSTM language model and stored the activations of all gates, the 2 hidden layers and memory cells of the model in a pickled dictionary.
Aside from the sentence and the corresponding activations, this dictionary contains also the position, log probability, syntactic depth, frequency count and open nodes count for each word in this sentence, and the length and structure of the sentence.

Since syntactic depth is naturally correlated with word position, we filtered the processed words such that all position-depth combinations within positions 7-12 and depths 3-8 are uniformly represented in our final dataset.
Note that the datapoints for our regression analysis are thus (word activation, depth) pairs, where not necessarily all words from one sentence in the original dataset are included.
Our final dataset contains 18.605 positions from 23.450 sentences.

We provide the script that generates these sentences as well as the files containing the exact sentences and filtered data we used for our study.

\end{document}
