\section{Summary and discussion}
We provided the first  detailed description of the underlying mechanism by which a language-model-trained LSTM performs long-distance  number agreement. %
%NA-tasks are of particular interest given that they require the network to capture structural relations in sentences.
Strikingly, simply training an LSTM with the language-model objective on raw corpus data brought about single units carrying exceptionally specific linguistic information. Three of these units were found to form a highly interactive local network, which makes up the central part of a `neural' circuit performing long-distance number agreement.

One of these units encodes and stores grammatical number information
when the main subject of a sentence is singular, and it successfully
carries this information across long-range dependencies. Another unit
similarly encodes plurality. These number units show that a highly
local encoding of linguistic features can emerge in LSTMs during
language-model training, as was previously suggested by theoretical
studies of artificial neural networks \cite[e.g.][]{Bowers:2009} and in
neuroscience \cite[e.g.,][]{Kutter:etal:2018}.

Our analysis also identified units whose activity correlates with syntactic complexity. These units, as a whole, affect performance on the agreement tasks. We found that one of them, more specifically, encodes the main subject-verb dependency across various syntactic constructions. Moreover, the highest afferent weights to the forget and input gates of both LR-number units were from this unit. A natural interpretation is that this unit propagates syntax-based remember and update flags that control when the number units store and release information.

Finally, number is also redundantly encoded in a more distributed way, but the latter mechanism is unable to carry information across embedded syntactic structures. The computational burden of tracking number information thus gave rise to two types of units in the network,  encoding similar information  with distinct properties and dynamics.

The relationship we uncovered and characterized between syntax and number units suggests that agreement in language-model-trained LSTMs cannot be entirely explained away by superficial heuristics, and the networks have, to some extent, learned to build and exploit structure-based syntactic representations, akin to those conjectured to support human-sentence processing.

%akin to those linguists conjecture support human sentence processing.

We hope that our study will inspire more analyses of the inner dynamics of LSTM and other sequence-processing networks, complementing the currently popular ``black-box probing'' approach. Besides bringing about a mechanistic understanding of language processing in artificial models, this could inform work on human-sentence processing. Indeed, our study yields particular testable predictions on brain dynamics, given that the computational burden of long-distance agreement remains the same for artificial and biological neural network. We conjecture a similar distinction between SR and LR units to be found in the human brain, as well as an interaction between syntax-processing and feature-carrying units such as the LR units, and plan to test this in future work.

% More generally, this work has several contributions. First, it provides the most detailed analysis of the inner dynamics of an LSTM performing a linguistic task we are aware of. We think that similar studies should complement currently popular black-box tests, to achieve a true understanding of language processing in neural networks. \marco{Here, we might want to mention the need for future work on relative clauses, as well as on other  models, such as transformers.}. Second, our simulations generated two testable predictions and can thus inform future work in neuroscience as discussed above. Finally, our findings about number agreement suggest that LSTM-LMs can encode the underlying subject-verb dependency across many syntactic structures without relying on canny heuristics. This contributes explicit evidence to the debate about structure-based processes in LSTM-LMs.



%\yair{perhaps for the longer version \sout{One speculative but intriguing possibilty from computational neuroscience is that the two identified types of number encoding would match two distinct types of neural mechanisms that were suggested as neural correlate of working memory in the literature. The first consists of explicit coding, which is based on enhanced spiking activity during information storage (cite Fuster:science:1971, Sompolinsky:NatureNeuro:2003) and could be seen as a candidate for LR dependencies with inferfering nouns. The second consists of a latent code, residing only in 'silent' pre-synaptic vesicle dynamics - spiking activity following an incoming stimulus leaves its 'imprint' onto the pre-synaptic neurons, and can be later recovered in the presence of non-specific inputs, and could be as a candidate for SR dependencies.}} 

% Intriguingly, we observed differential handling of singular and plural information, with the latter being more reliably processed, resulting in more robust plural agreement. As a similar asymmetry has also been observed in humans (\textbf{REFS}), one exciting avenue for future work might consist in looking for different singular/plural processing networks in the human brain.
