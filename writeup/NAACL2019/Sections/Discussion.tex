
\section{Discussion}
This work provides a detailed description of the underlying mechanism by which an LSTM-LM performs number-agreement tasks. NA-tasks are of particular interest given that they require the network to capture structural relations in sentences. Strinkingly, training an LSTM on the language-model objectif with unannotated corpus data was found to bring about single units that carry exceptionally specific roles, three of which were found to form a highly interactive local network, which makes up the central part of a 'neural' circuit that solves the NA-task, which the LSTM was not explicitely trained to solve.

One of these units was found to encode and store grammatical-number information only if the main subject of the sentence was singular, and was found to succesfully carry this information beyond interfering plural nouns and across long-range (LR) dependencies. A plural unit, its mirror counterpart, was similarly found to exist in the network. These two specialized LR-number units show that a highly local encoding of linguistic features can emerge in LSTM-LM during training, as was previously suggested by studies on neural networks (e.g., Bowers) and in neuroscience (Kutter:Neuron:2018)

Our analysis has also identifed units whose activity well correlated with syntactic complexity. One of this syntax units was shown to encode the main subject-verb dependency in various syntactic constructions. Moreover, among its highest effernt weights two were to the LR-numbers, both to their forget and input gates, with signs that suggest an apparent interpretation - propagating to the LR umber-units a 'remembering' signal throughout the subject-verb dependency and an 'updating' signal at the end of it. Together, the LR-number units and the syntax unit form a main component of the circuit underlying LSTM capacity in performing the NA-task. We hope that this kind of granular understanding of how LSTMs perform number agreement will inform work on neural data on sentence processing, as we conjecture a similar interaction between syntactic and feature-carrying units might be implemented in the human brain.

Our analysis also identified another type of number units in the network, which encode and store grammatical number for short-range (SR) dependencies but cannot steadily carry this information beyond interfering nouns. Moreover, qualitatively, gate and cell dyanmics of these SR-number units are considerably different than that of the LR ones \footnote{Reproting this in details is beyond paper space limitations.}, and the relatively large number of SR units suggest that they encode grammatical number in a more distributed way. It therefore shows that the computational burden of the NA-task gave rise to two types of units in the network that encode similar information content but have distinct properties and dynamics. This yields a particular testable prediction on brain dynamics, given that the computational burden of the NA-task remains the same for artifical and biological neural networks. That is, it suggest the possible existence of two such distinct type of units also in the brain.

More generally, this work has several contributions. First, it provides the most detailed analysis of the inner dynamics of an LSTM performing a linguistic task we are aware of. We think that similar studies should complement currently popular black-box tests, to achieve a true understanding of language processing in neural networks. \textbf{Here, we might want to mention the need for future work on relative clauses, as well as on other  models, such as transformers.}. Second, our simulations generate two testable predictions and can thus inform future work in neuroscience as discussed above. Finally, our findings about number agreement suggest that LSTM-LMs can encode the underlying subject-verb dependency across many syntactic structures without relying on canny heuristics. This contributes explicit evidence to the debate about strctured-based processes in sentence comprehension.


%\yair{perhaps for the longer version \sout{One speculative but intriguing possibilty from computational neuroscience is that the two identified types of number encoding would match two distinct types of neural mechanisms that were suggested as neural correlate of working memory in the literature. The first consists of explicit coding, which is based on enhanced spiking activity during information storage (cite Fuster:science:1971, Sompolinsky:NatureNeuro:2003) and could be seen as a candidate for LR dependencies with inferfering nouns. The second consists of a latent code, residing only in 'silent' pre-synaptic vesicle dynamics - spiking activity following an incoming stimulus leaves its 'imprint' onto the pre-synaptic neurons, and can be later recovered in the presence of non-specific inputs, and could be as a candidate for SR dependencies.}} 

% Intriguingly, we observed differential handling of singular and plural information, with the latter being more reliably processed, resulting in more robust plural agreement. As a similar asymmetry has also been observed in humans (\textbf{REFS}), one exciting avenue for future work might consist in looking for different singular/plural processing networks in the human brain.
