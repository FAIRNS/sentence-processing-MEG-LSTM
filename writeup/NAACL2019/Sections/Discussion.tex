
\section{Discussion}
We described a detailed analysis of the underlying mechanism of an LSTM-LM performing various number-agreement (NA) tasks. Training the LSTM-LM on unannotated corpus data was found to bring about the emergence of units that carry exceptionally specific roles, three of them were found to form a highly interactive local network, which makes a central part in the underlying 'neural' circuit solving the NA-task. 

One of these units was found to encode and store grammatical-number information only if the main subject of the sentence was singular, and was found to succesfully carry this information beyond interfering plural nouns across long-range (LR) dependencies. A plural unit, its mirror counterpart, was similarly found to exist in the network. These two exceptionally specialized LR-number units show that a highly local encoding of linguistic features can spontateously emerge in LSTM-LM during training, as was previously suggested by preivous studies on neural networks (e.g., Bowers) and from neuroscience (Kutter:Neuron:2018)

Another type of number units was identified in the network, of units that encode and store grammatical number for short-range (SR) dependencies but cannot steadily carry this information beyond interfering nouns. Moreover, gate and cell dyanmics of these SR-number units were shown to be different than that of the LR ones and to encode grammatical number in a more distributed way. This contrast between SR and LR-number units suggests that the computational burden of the NA-task gave rise to two types of units in the network, which encode similar information content but have distinct properties and dynamics. This finding yields a particular testable prediction on brain dynamics, given that the computational burden of the NA-task remains the same for artifical and biological neural networks. That is to say, our findings suggest that two such distinct types of storing of number information may be found also in the brain - a local and a more distributed one. \yair{perhaps for the longer version \sout{One speculative but intriguing possibilty from computational neuroscience is that the two identified types of number encoding would match two distinct types of neural mechanisms that were suggested as neural correlate of working memory in the literature. The first consists of explicit coding, which is based on enhanced spiking activity during information storage (cite Fuster:science:1971, Sompolinsky:NatureNeuro:2003) and could be seen as a candidate for LR dependencies with inferfering nouns. The second consists of a latent code, residing only in 'silent' pre-synaptic vesicle dynamics - spiking activity following an incoming stimulus leaves its 'imprint' onto the pre-synaptic neurons, and can be later recovered in the presence of non-specific inputs, and could be as a candidate for SR dependencies.}} 

Finally, we identifed 


More generally, we provided the most detailed analysis of the inner
dynamics of an LSTM performing a linguistic task we are aware of. We
think that similar studies should complement currently popular
black-box tests, to achieve a true understanding of language
processing in neural networks. \textbf{Here, we might want to mention
  the need for future work on relative clauses, as well as on other
  models, such as transformers.}

From a more cognitive perspective, we hope our granular
understanding of how LSTMs perform number agreement will inform work
on computational modeling of neural sentence processing data, as we
conjecture a similar interaction between syntactic and
feature-carrying units might be implemented in the human
brain. Intriguingly, we observed differential handling of singular and
plural information, with the latter being more reliably processed,
resulting in more robust plural agreement. As a similar asymmetry has
also been observed in humans (\textbf{REFS}), one exciting avenue for
future work might consist in looking for different singular/plural
processing networks in the human brain.
