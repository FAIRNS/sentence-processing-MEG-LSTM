
\section{Discussion}
We described a detailed analysis of the underlying mechanism of an LSTM-LM performing various number-agreement (NA) tasks. Training the LSTM-LM on unannotated corpus data was found to bring about single units that carry exceptionally specific roles, three of which were found to form a highly interactive local network, which makes up the central part in the underlying 'neural' circuit that solves the NA-task. 

One of these units was found to encode and store grammatical-number information only if the main subject of the sentence was singular, and was found to succesfully carry this information beyond interfering plural nouns and across long-range (LR) dependencies. A plural unit, its mirror counterpart, was similarly found to exist in the network. These two specialized LR-number units show that a highly local encoding of linguistic features can emerge in LSTM-LM during training, as was previously suggested by studies on neural networks (e.g., Bowers) and in neuroscience (Kutter:Neuron:2018)

Another type of number units was identified in the network, of units that encode and store grammatical number for short-range (SR) dependencies but cannot steadily carry this information beyond interfering nouns. Moreover, gate and cell dyanmics of these SR-number units were shown to be different than that of the LR ones and to encode grammatical number in a more distributed way. This contrast between SR and LR-number units suggests that the computational burden of the NA-task gave rise to two types of units in the network that encode similar information content but have distinct properties and dynamics. This finding yields a particular testable prediction on brain dynamics, given that the computational burden of the NA-task remains the same for artifical and biological neural networks. That is, it suggest the possible existence of two such distinct type of units also in the brain \yair{I should probably have hedged this due to implementational and possible algorithmic differences?}.
%\yair{perhaps for the longer version \sout{One speculative but intriguing possibilty from computational neuroscience is that the two identified types of number encoding would match two distinct types of neural mechanisms that were suggested as neural correlate of working memory in the literature. The first consists of explicit coding, which is based on enhanced spiking activity during information storage (cite Fuster:science:1971, Sompolinsky:NatureNeuro:2003) and could be seen as a candidate for LR dependencies with inferfering nouns. The second consists of a latent code, residing only in 'silent' pre-synaptic vesicle dynamics - spiking activity following an incoming stimulus leaves its 'imprint' onto the pre-synaptic neurons, and can be later recovered in the presence of non-specific inputs, and could be as a candidate for SR dependencies.}} 

Our analysis has also identifed units whose activity well correlated with syntactic complexity, measured by the transient number of open nodes in the syntactic tree. One of this syntax units was shown to encode the presence of an embedded phrase within the main subject-verb dependency. Moreover, this unit had exceptionally high effernt weights to the two LR-numbers, and was shown to control the forget and input gates of the LR-units in an unmistakable interpretation - conveying a 'remembering' signal from after the main subject and throughout the subject-verb dependency, and an 'updating' signal when the main verb appears at the end of the dependency. From a more cognitive perspective, we hope that here too our granular understanding of how LSTMs perform number agreement will inform work on neural data on sentence processing, as we conjecture a similar interaction between syntactic and feature-carrying units might be implemented in the human brain.


More generally, this work provides the most detailed analysis of the inner dynamics of an LSTM performing a linguistic task we are aware of. We think that similar studies should complement currently popular black-box tests, to achieve a true understanding of language processing in neural networks. \textbf{Here, we might want to mention the need for future work on relative clauses, as well as on other  models, such as transformers.}


% Intriguingly, we observed differential handling of singular and plural information, with the latter being more reliably processed, resulting in more robust plural agreement. As a similar asymmetry has also been observed in humans (\textbf{REFS}), one exciting avenue for future work might consist in looking for different singular/plural processing networks in the human brain.
