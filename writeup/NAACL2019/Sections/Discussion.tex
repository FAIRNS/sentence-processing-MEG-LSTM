
\section{Summary and Discussion}
This work provided a detailed description of the underlying mechanism by which a language-model-trained LSTM performs long-distance  number agreement. %
%NA-tasks are of particular interest given that they require the network to capture structural relations in sentences.
Strikingly, simply training an LSTM with the language-model objective on unannotated corpus data brought about single units carrying exceptionally specific linguistic information. Three of these units were found to form a highly interactive local network, which makes up the central part of a `neural' circuit performing long-distance number agreement.

One of these units encodes and stores grammatical number information when the main subject of a sentence is singular, and it successfully carries this information across long-range dependencies. Another unit similarly encodes plural information. These number units show that a highly local encoding of linguistic features can emerge in LSTMs during language-model training, as was previously suggested by studies on neural networks (e.g., Bowers) and in neuroscience (Kutter:Neuron:2018).

Our analysis also identified units whose activity correlates with syntactic complexity. These units, as a whole, affect performance on the agreement tasks. We found that one of them, more specifically, encodes the main subject-verb dependency across various syntactic constructions. Moreover, two of its highest efferent weights were to the number units forget and input gates. A natural interpretation is that this unit propagates syntax-based 'remember' and 'update' flags that control when the number units store and release information. % Together, the LR-number units and the syntax unit thus form the main component of the circuit underlying LSTM capacity in performing the NA-task.

Our analysis also identified units encoding number  but unable to steadily carry  information beyond interfering nouns. Qualitatively, gate and cell dynamics of these SR-number units are considerably different than that of the LR ones \footnote{A full report is beyond the scope of this paper.}, and the relatively large number of SR units suggest that they encode grammatical number in a more distributed way. It therefore shows that the computational burden of the NA-task gave rise to two types of units in the network that encode similar information content but have distinct properties and dynamics. This yields a particular testable prediction on brain dynamics, given that the computational burden of the NA-task remains the same for artificial and biological neural networks - we conjecture the existence of two such distinct types of number-information encoding also in the brain.

More generally, this work has several contributions. First, it provides the most detailed analysis of the inner dynamics of an LSTM performing a linguistic task we are aware of. We think that similar studies should complement currently popular black-box tests, to achieve a true understanding of language processing in neural networks. \marco{Here, we might want to mention the need for future work on relative clauses, as well as on other  models, such as transformers.}. Second, our simulations generated two testable predictions and can thus inform future work in neuroscience as discussed above. Finally, our findings about number agreement suggest that LSTM-LMs can encode the underlying subject-verb dependency across many syntactic structures without relying on canny heuristics. This contributes explicit evidence to the debate about structure-based processes in LSTM-LMs.


We hope that this kind of granular understanding of how LSTMs perform number agreement will inform work on neural data on sentence processing, as we conjecture a similar interaction between syntactic and feature-carrying units might be implemented in the human brain.

%\yair{perhaps for the longer version \sout{One speculative but intriguing possibilty from computational neuroscience is that the two identified types of number encoding would match two distinct types of neural mechanisms that were suggested as neural correlate of working memory in the literature. The first consists of explicit coding, which is based on enhanced spiking activity during information storage (cite Fuster:science:1971, Sompolinsky:NatureNeuro:2003) and could be seen as a candidate for LR dependencies with inferfering nouns. The second consists of a latent code, residing only in 'silent' pre-synaptic vesicle dynamics - spiking activity following an incoming stimulus leaves its 'imprint' onto the pre-synaptic neurons, and can be later recovered in the presence of non-specific inputs, and could be as a candidate for SR dependencies.}} 

% Intriguingly, we observed differential handling of singular and plural information, with the latter being more reliably processed, resulting in more robust plural agreement. As a similar asymmetry has also been observed in humans (\textbf{REFS}), one exciting avenue for future work might consist in looking for different singular/plural processing networks in the human brain.
