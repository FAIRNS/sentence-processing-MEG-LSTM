\section{Data}\label{sec:the_data}

\paragraph{Number-agreement tasks}
\begin{table}[tb]
  \centering
  \begin{footnotesize}
  \begin{tabular}{l@{\hskip1pt}l}
    \B Simple & the \textbf{boy} \textbf{greets} the guy\\
    \B Adv & the \textbf{boy} probably \textbf{greets} the guy\\
    \B 2Adv & the \textbf{boy} most probably \textbf{greets} the guy\\
    \B CoAdv &  the \textbf{boy} openly and deliberately \textbf{greets} the guy\\
    \B NamePP & the \textbf{boy} near Pat \textbf{greets} the guy\\
    \B NounPP & the \textbf{boy} near the car \textbf{greets} the guy\\
    \B NounPPAdv & the \textbf{boy} near the car kindly \textbf{greets} the guy\\
    %\B SubjRel & the boy that avoids the girl greets the guy\\
    %\B ObjRel  & the boy that the girl avoids greets the guy \\
    %\B ObjRel0 &  the boy the girl avoids greets the guy\\
  \end{tabular}
  \end{footnotesize}
  \caption{NA tasks illustrated by representative
    singular sentences.}
  \label{tab:data-sets}
\end{table}

We complement analysis of the naturalistic, corpus-derived
number-agreement test set of \newcite{Linzen:etal:2016}, in the
version made available by \newcite{Gulordava:etal:2018}, with
synthetically generated data-sets. Each synthetic number-agreement
task (NA-task) instantiates a fixed syntactic structure with varied
lexical material, in order to probe subject-verb number agreement in
controlled and increasingly challenging setups.\footnote{We exclude,
  for the time being, agreement across a relative clause, as it comes
  with the further complication of accounting for the extra agreement
  process taking place inside the relative clause. \yair{add: and low performance for objrel? Eventually we do look at subjrel and double-subjrel but only for 1150}}

The different structures are illustrated in Table \ref{tab:data-sets}
by examples where all forms are in the singular. Distinct sentences
were randomly generated by selecting (different) words from pools of
20 subject/object nouns, 15 verbs, 10 adverbs, 5 prepositions, 10
proper nouns and 10 location nouns. The items were selected so that
their combination would not lead to semantic anomalies. For each
NA-task, we generated singular and plural versions of each
sentence. We refer to each such version as a \textit{condition}. For
NA-tasks that have other nouns occurring between subject and main
verb, we also systematically vary their number, resulting in two
\textit{congruent} and two \textit{incongruent} conditions. For example, the NounPP
sentence in the table illustrates the congruent SS (singular-singular)
condition and the corresponding sentence in the incongruent PS (plural-singular)
condition is: ``the \emph{boys} near the \emph{car} \emph{greet} the guy''. For all
NA-tasks, each condition consisted of 600 sentences

\paragraph{Syntactic depth data-set} We probed the model implicit
syntax parsing abilities by testing whether its representations
predict the syntactic depth of the words they process. Following
\newcite{Nelson:etal:2017}, this was operationalized as predicting the
number of open syntactic nodes at each word, given the canonical
syntactic parse of a sentence.  We generated a data-set of sentences
with unambiguous but varied syntactic structures (for example: ``[$_S$
[$_{NP}$ the [$_{N''}$ [$_{N'}$ [$_{AP}$ really tiny ] actress ]
[$_{PP}$ behind Jim ] ] ] [$_{VP}$ fights [$_{NP}$ four [$_{N''}$
friends [$_{PP}$ of John ] ] ] ] ]''). Since syntactic depth is
naturally correlated with the position of a word in a sentence, we
used a data-point sampling strategy de-correlating these factors. For
each length between ????  and ???? words, we randomly generated 300
sentences. From this set, we randomly picked examples uniformly
covering all possible position-depth combinations within the 7-12
position and 3-8 depth ranges.  The final data-set contains ????
positions from ???? sentences. These were split into ???? training,
???? validation and ???? positions for the experiment detailed in
Section \ref{sec:syntax-units} below.\footnote{All our data-sets are
  uploaded as supplementary materials and will be openly released upon
  publication.} \textbf{Was the train/valid/test split by sentence or
  position?}

%\yair{perhaps to consider to move these explanations about contrasts to the results section:} %Note that 2Adv features the same subject-verb
%distance as NamePP, but without gender-carrying words acting as
%possible distractors in the middle. Similarly, CoAdv can serve as a
%distractor-free control for NounPP.
%Task sata-set sizes across conditions ranged from 600 (Simple) to 18k sentences (relatives),
%based on the possibilities for variation allowed by the combinatorics
%given each structure.
