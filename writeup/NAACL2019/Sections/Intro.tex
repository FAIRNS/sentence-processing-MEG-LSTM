\section{Introduction}

\yair{I love the description below. We could perhaps try to anchor the work within a slightly wider context, by first discussing the debate between structure-based vs. statistical/heuristic approachs to sentence processing in humans - so in linguistics (e.g., Chomskian/Pinker vs Gibson) and in neuroscience (e.g., Nelson et al vs. Fedorenko). Then, describing the recent findings about LSTM-LMs as a parallel debate about RNNs, as in the first paragraph below, and finally to stress the advantages of working out the latter debate about LSTM-LM through computational simulations, like our work, in contributing to insights regarding the original debate in humans, producing concrete predictions on brain dynamics. I'm therefore adding a paragraph but we could postpone this for the more 'cognitive' audience in the longer version.}

\yair{Sentence comprehension requires to process dependencies between words in the sentence. These dependencies may affect the meaning ascribed to sentence constituents and the prediction of next ones. Two opposing theoretical approches have been suggested for sentence comprehension. The first, more traditional one, suggests that structure-based syntactic processes are the principal computational capacity of human language (cite Chomsky 57,72; Pinker 95; Hauser et al 2002). According to this approach, word dependencies are described by syntactic trees and their processing by structure-based parsing computations (cite). The second, more recent approach, suggests that more fuzzy processes may underline human-language computations, embrasing 'good enough' parsing and canny heuristics (cite, eg, Traxler 2014; Ferreira, F., & Patson, N. D. 2007; ). According to this approach, word dependencies, and sentence comprehension in general, are desribed by information-based and Bayesian theories (e.g., Levy R, 2009; Gibson et al 2013). Following this, a parallel debate has emerged in the neuroscientific community. On the one hand, several studies provided evidence that brain processes correspond to traditional syntactic-theories descriptions (cite Friederici, 2006, 2013, Nelson 2017). On the other hand, other studies have provided evidence against the primacy of such syntactic and structure-based computations in neural processing of sentences (e.g., Fedorenko 2018). Interestingly, this debate has also recently emerged in the literature about sentence processing in artificial neural networks.}

In the last years, recurrent neural networks, and particularly
long-short-term-memory (LSTM) architectures
\cite{Hochreiter:Schmidhuber:1997}, have been successfully applied to
a variety of challenging NLP tasks. This has spurred interest in
whether these generic sequence-processing devices are discovering
genuine structural properties of language in their training data, or
whether their success can be explained by opportunistic
heuristics. Starting with the seminal work of
\newcite{Linzen:etal:2016}, long-distance number agreement (``the
\textbf{boy} behind the trees \textbf{greets}\ldots'') has played a
central role in this debate. After mixed initial results by Linzen and
colleagues and \newcite{Bernardy:Lappin:2017},
\newcite{Gulordava:etal:2018} and \newcite{Kuncoro:etal:2018a} have
robustly established that LSTMs trained with the language modeling
objective on raw data achieve near-human performance on the
agreement task. While Gulordava and colleagues provided some evidence that the
LSTMs are relying on genuine syntactic generalizations,
\newcite{Kuncoro:etal:2018b} and \newcite{Linzen:Leonard:2018}
suggested that the LSTM achievements can, at least in part, be
accounted by superficial heuristics.

Until now, the debate has rested on ``behavioural'' evidence: The
LSTM is treated as a black box, and its capacities are indirectly
inferred by its performance on linguistic tasks. We take here a
complementary ``neuroscientific'' approach: We thoroughly investigate
the inner dynamics of an LSTM performing the number agreement task,
striving to achieve a mechanistic understanding of how it accomplishes
it.

We find that the LSTM specialized very few ``grandma'' cells
\cite{Bowers:2009} to carry number features from the subject to the
verb across the intervening material in a highly localist
fashion. Interestingly, the LSTM also possesses a more distributed
mechanism to predict number when subject and verb are close, with the
grandma number cells only playing a crucial role in more difficult
long-distance cases. Crucially, we independently identified a set of
cells tracking syntactic structure, and found that one of them \yair{encodes the presence of an embedded phrase separating the main subject-verb dependency, and has} strong efferent connections to the long-distance number cells,
suggesting that the network relies on genuine syntactic information to
regulate agreement-feature percolation.

Our analysis thus provides direct evidence for the claim that LSTMs
trained with language modeling on unannotated corpus data, despite
lacking significant linguistic priors, learn to perform
structure-dependent linguistic operations. In turn, this suggests that
raw linguistic input and generic memory mechanisms, such as those
implemented in LSTMs, may suffice to trigger the induction of
non-trivial grammatical rules.
