\section{Introduction}

\yair{I love the intro as is. We could perhaps try to anchor the work within a slightly wider context though, by first discussing the debate between structure-based vs. statistical/heuristic approachs to sentence processing in humans in both linguistics and neuroscience. Then, describing the recent findings about LSTM-LMs as a parallel debate about RNNs, and finally to stress the advantages of working out the latter debate about LSTM-LM through computational simulations, like our work, in contributing to insights regarding the original debate in humans. I therefore sketched a possible intro paragraph, but we could postpone this for the more 'cognitive' audience in the longer version (and perhaps its also anyway too ambitious to describe our work as contributing in any way to the classic debate).}

\yair{Sentence comprehension requires to process dependencies between words in the sentence. Such dependencies may affect the meaning ascribed to sentence constituents and the prediction of next ones. Two opposing theoretical approches have been suggested for sentence comprehension. The first, more traditional one, suggests that structure-based syntactic processes are the principal computational capacity of human language (cite Chomsky 57,72; Pinker 95; Hauser et al 2002). According to this approach, word dependencies are described by syntactic trees and their processing is described by structure-based parsing computations (cite). The second, more recent approach, suggests that more fuzzy processes may underline human-language computations, thus embrasing 'good enough parsing' and canny heuristics (cite, eg, Traxler 2014; Ferreira, F., & Patson, N. D. 2007; ). According to this approach, word dependencies, and sentence comprehension in general, are instead desribed by information-based and Bayesian theories (e.g., Levy R, 2009; Gibson et al 2013). Following this, a parallel debate has emerged in the neuroscientific community, drawing into consideration also the continuous dyanmics of biological nerual network akin to fuzzy computations. On the one hand, several studies provided evidence that brain processes lend themselves to interpreations from traditional syntax theories (cite Friederici, 2006, 2013, Nelson 2017). On the other hand, other studies have provided evidence against the primacy of such syntactic and structure-based computations in neural processing of sentences (e.g., Fedorenko 2018). Interestingly, recently, a similar debate about sentence processing has emerged about state-of-the-art sequential-learning models in the field of artificial neural networks (cite), thus awakening an older one (cite fodor \& pylyshyn). Since both artifical and biological neural networks preserve the tension between their continuous dynamics and the discrete, structure-based, character of syntacic processing, and given the flexibilty in conducting experiments with ANNs and recent advances in the field, looking into this debate from this perspective could therefore be in particular instructional.}

In the last years, recurrent neural networks, and particularly
long-short-term-memory (LSTM) architectures
\cite{Hochreiter:Schmidhuber:1997}, have been successfully applied to
a variety of challenging NLP tasks. This has spurred interest in
whether these generic sequence-processing devices are discovering
genuine structural properties of language in their training data, or
whether their success can be explained by opportunistic
heuristics\yair{, by e.g. overfitting to simple patterns that are predictive in most cases and which do not require deeper, structure-based, language understanding}. Starting with the seminal work of \newcite{Linzen:etal:2016}, long-distance number agreement (``the
\textbf{boy} behind the trees \textbf{greets}\ldots'') has played a
central role in this debate. After mixed initial results by Linzen and
colleagues and \newcite{Bernardy:Lappin:2017},
\newcite{Gulordava:etal:2018} and \newcite{Kuncoro:etal:2018a} have
robustly established that LSTMs trained with the language modeling
objective on raw data achieve near-human performance on the
agreement task. While Gulordava and colleagues provided some evidence that the
LSTMs are relying on genuine syntactic generalizations,
\newcite{Kuncoro:etal:2018b} and \newcite{Linzen:Leonard:2018}
suggested that the LSTM achievements can, at least in part, be
accounted by superficial heuristics.

Until now, the debate has rested on ``behavioural'' evidence: The
LSTM is treated as a black box, and its capacities are indirectly
inferred by its performance on linguistic tasks. We take here a
complementary ``neuroscientific'' approach: We thoroughly investigate
the inner dynamics of an LSTM performing the number agreement task,
striving to achieve a mechanistic understanding of how it accomplishes
it.

We find that the LSTM specialized very few ``grandma'' cells
\cite{Bowers:2009} to carry number features from the subject to the
verb across the intervening material in a highly localist
fashion. Interestingly, the LSTM also possesses a more distributed
mechanism to predict number when subject and verb are close, with the
grandma number cells only playing a crucial role in more difficult
long-distance cases. Crucially, we independently identified a set of
cells tracking syntactic structure, and found that one of them \yair{encodes the presence of an embedded phrase separating the main subject-verb dependency, and has} strong efferent connections to the long-distance number cells,
suggesting that the network relies on genuine syntactic information to
regulate agreement-feature percolation.

Our analysis thus provides direct evidence for the claim that LSTMs
trained with language modeling on unannotated corpus data, despite
lacking significant linguistic priors, learn to perform
structure-dependent linguistic operations. In turn, this suggests that
raw linguistic input and generic memory mechanisms, such as those
implemented in LSTMs, may suffice to trigger the induction of
non-trivial grammatical rules.
