\section{Introduction}

In the last years, recurrent neural networks (RNNs), and particularly
long-short-term-memory (LSTM) architectures
\cite{Hochreiter:Schmidhuber:1997}, have been successfully applied to
a variety of NLP tasks. This has spurred interest in whether these
generic sequence-processing devices are discovering genuine structural
properties of language in their training data, or whether their
success can be explained by opportunistic surface-pattern-based
heuristics.

Until now, this debate has mostly relied on ``behavioural'' evidence:
The LSTM had been treated as a black box, and its capacities had been indirectly
inferred by its performance on linguistic tasks. In this study, we took a
complementary ``neuroscientific'' approach: We thoroughly investigated
the inner dynamics of an LSTM language model performing the number agreement task,
striving to achieve a mechanistic understanding of how it accomplishes
it. We found that the LSTM had specialized two ``grandmother'' cells
\cite{Bowers:2009} to carry number features from the subject to the
verb across the intervening material.\footnote{In the neuroscientific
  literature, ``grandmother'' cells are (sets of) neurons coding for
  specific information, e.g., about your grandmother, in a
  non-distributed manner.} Interestingly, the LSTM also
possesses a more distributed mechanism to predict number when subject
and verb are close, with the grandmother number cells only playing a
crucial role in more difficult long-distance cases. Crucially, we
independently identified a set of cells tracking syntactic structure,
and found that one of them encodes the presence of an embedded phrase
separating the main subject-verb dependency, and has strong efferent
connections to the long-distance number cells, suggesting that the
network relies on genuine syntactic information to regulate
agreement-feature percolation.

Our analysis thus provides direct evidence for the claim that LSTMs
trained on unannotated corpus data, despite
lacking significant linguistic priors, learn to perform
structure-dependent linguistic operations. In turn, this suggests that
raw linguistic input and generic memory mechanisms, such as those
implemented in LSTMs, may suffice to trigger the induction of
non-trivial grammatical rules.