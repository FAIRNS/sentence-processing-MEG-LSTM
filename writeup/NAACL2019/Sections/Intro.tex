\section{Introduction}

In the last years, recurrent neural networks (RNNs), and particularly
long-short-term-memory (LSTM) architectures
\cite{Hochreiter:Schmidhuber:1997}, have been successfully applied to
a variety of NLP tasks. This has spurred interest in whether these
generic sequence-processing devices are discovering genuine structural
properties of language in their training data, or whether their
success can be explained by opportunistic surface-pattern-based
heuristics.

Until now, this debate has mostly relied on ``behavioural'' evidence:
The LSTM is treated as a black box, and its capacities are indirectly
inferred by its performance on linguistic tasks. We take here a
complementary ``neuroscientific'' approach: We thoroughly investigate
the inner dynamics of an LSTM performing the number agreement task,
striving to achieve a mechanistic understanding of how it accomplishes
it. We find that the LSTM specialized two ``grandma'' cells
\cite{Bowers:2009} to carry number features from the subject to the
verb across the intervening material.\footnote{In the neuroscientific
  literature, ``grandma'' cells are (sets of) neurons coding for
  specific information, e.g., about your grandomther, in a
  non-distributed manner.} Interestingly, the LSTM also
possesses a more distributed mechanism to predict number when subject
and verb are close, with the grandma number cells only playing a
crucial role in more difficult long-distance cases. Crucially, we
independently identified a set of cells tracking syntactic structure,
and found that one of them encodes the presence of an embedded phrase
separating the main subject-verb dependency, and has strong efferent
connections to the long-distance number cells, suggesting that the
network relies on genuine syntactic information to regulate
agreement-feature percolation.

Our analysis thus provides direct evidence for the claim that LSTMs
trained with language modeling on unannotated corpus data, despite
lacking significant linguistic priors, learn to perform
structure-dependent linguistic operations. In turn, this suggests that
raw linguistic input and generic memory mechanisms, such as those
implemented in LSTMs, may suffice to trigger the induction of
non-trivial grammatical rules.

%\yair{In case we would need further compression in the intro, we could consider omitting the penultimate paragraph that describes the results, and leave the reader with only the teaser in the current last paragraph.}


%\marco{Yair, I think your intro is excellent, but I feel is too long for a NAACL conference paper, where probably even mine will have to be shortened: I'd comment it out and keep it for the journal version.}

%\yair{I love the intro as is. We could perhaps try to anchor the work within a slightly wider context though, by first discussing the debate between structure-based vs. statistical/heuristic approachs to sentence processing in humans in both linguistics and neuroscience. Then, describing the recent findings about LSTM-LMs as a parallel debate about RNNs, and finally to stress the advantages of working out the latter debate about LSTM-LM through computational simulations, like our work, in contributing to insights regarding the original debate in humans. I therefore sketched a possible intro paragraph, but we could postpone this for the more 'cognitive' audience in the longer version (and perhaps its also anyway too ambitious to describe our work as contributing in any way to the classic debate).}

%\yair{Sentence comprehension requires to process dependencies between words in the sentence. Such dependencies may affect the meaning ascribed to sentence constituents and the prediction of next ones. Two opposing theoretical approches have been suggested for sentence comprehension. The first, more traditional one, suggests that structure-based syntactic processes are the principal computational capacity of human language (cite Chomsky 57,72; Pinker 95; Hauser et al 2002). According to this approach, word dependencies are described by syntactic trees and their processing is described by structure-based parsing computations (cite). The second, more recent approach, suggests that more fuzzy processes may underline human-language computations, thus embrasing 'good enough parsing' and canny heuristics (cite, eg, Traxler 2014; Ferreira, F., \& Patson, N. D. 2007; ). According to this approach, word dependencies, and sentence comprehension in general, are instead desribed by information-based and Bayesian theories (e.g., Levy R, 2009; Gibson et al 2013). Following this, a parallel debate has emerged in the neuroscientific community, drawing into consideration also the continuous dyanmics of biological nerual network akin to fuzzy computations. On the one hand, several studies provided evidence that brain processes lend themselves to interpreations from traditional syntax theories (cite Friederici, 2006, 2013, Nelson 2017). On the other hand, other studies have provided evidence against the primacy of such syntactic and structure-based computations in neural processing of sentences (e.g., Fedorenko 2018). Interestingly, recently, a similar debate about sentence processing has emerged about state-of-the-art sequential-learning models in the field of artificial neural networks (eg Linzen et al 2016, Gulordava:etal:2018, Kuncoro:etal:2018b, Linzen:Leonard:2018; see also Lake B et al:Behav and Brain sci:2017 for a related, more general, scope of debate, awakening an older one fodor\&pylyshyn:1988). Since both artifical and biological neural networks preserve the tension between their continuous dynamics and the discrete, structure-based, character of syntacic processing, while ANNs provide flexibility in conducting experiments, also going through tremendous recent advances, looking into this debate from this perspective may turn out to be in particular instructional.}

