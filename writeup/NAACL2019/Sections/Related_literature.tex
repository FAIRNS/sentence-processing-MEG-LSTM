
\section{Related work}

% \marco{This can be made more compact by removing the intro par, and in the behavioural part only discussing studies of agreement. I would remove the part about which studies suggested agreement is done via heuristics and which argued it's genuinely structure-sensitive from the intro, and move it here. Happy to do that myself.}

% Previous studies on understanding RNNs can be roughly characterized in two classes: studies that focus on visualising or interpreting the hidden state activations of such networks; and work that focuses on the behavior or performance of a model by studying the learnability of particular datasets or phenomena.
% We briefly discuss the most important studies concerning RNNs applied for natural language processing.
% For the second class, we specifically focus on behavioral studies concerning \textit{neural language models},a topic particularly relevant for the current paper.

Starting with the seminal work of \newcite{Linzen:etal:2016},
long-distance number agreement (``the \textbf{boy} near the cars
\textbf{greets}\ldots'') has emerged as a standard way to probe the
syntactic capabilities of RNNs. After
mixed initial results by Linzen and colleagues and
\newcite{Bernardy:Lappin:2017}, \newcite{Gulordava:etal:2018} and
\newcite{Kuncoro:etal:2018a} have robustly established that LSTMs
trained with the language modeling objective on raw data achieve
near-human performance on the agreement task. While Gulordava and
colleagues provided some evidence that the LSTMs are relying on
genuine syntactic generalizations, \newcite{Kuncoro:etal:2018b} and
\newcite{Linzen:Leonard:2018} suggested that the LSTM achievements
can, at least in part, be accounted by superficial heuristics (e.g., ``percolate the number of the first noun in a sentence''). Other
recent work has extended syntax probing to other phenomena such as
negative polarity items and island constraints
\cite{Chowdhury:Zamparelli:2018,jumelet2018language,marvin2018targeted,wilcox2018rnn}.

While \newcite{Linzen:etal:2016} presented intriguing qualitative data
showing cells that track number in a network specifically trained on
the agreement task, most work of this sort focuses on testing the
network output behaviour, rather than on understanding how the latter
follows from the network inner representations. Another research line
studies linguistic processing in neural networks through `diagnostic classifiers', that is, classifiers trained to
predict a certain property from network activations
\cite[e.g.,][]{gelderloos2016phonemes,Adi:etal:2017,alain2017understanding,Hupkes:etal:2017}. This
approach can shed insights on which information is encoded by the
network in different layers or at different time points, but it only
provides indirect evidence about the specific mechanics of linguistic
processing in the network.
% E.g., \newcite{gelderloos2016phonemes} find evidence that a multilayer
% RNN trained in a visually grounded learning paradigm learns to
% represent linguistic information in a hierarchy, encoding form in the
% lower and meaning in the higher layers.  \dieuwke{This is of course
%   not very extensive, but maybe this is not really the place to put a
%   too elaborate analysis?}

Other studies are closer to our approach in that they attempt to
attribute function to specific network cells, often by means of
visualization
\cite{Karpathy:etal:2016,li2016visualizing,tang2017memory}. \newcite{Radford:etal:2017},
for example, detected a ``sentiment'' grandmother cell in a
language-model-trained network.  \newcite{Kementchedjhieva:Lopez:2018}
recently found a character-level RNN to track morpheme boundaries in a
single cell. We are however not aware of others studies systematically
characterizing the processing of a linguistic phenomenon at the level of
RNN cell dynamics as we attempt to do here.


% E.g., \newcite{gelderloos2016phonemes} find evidence that a multilayer
% RNN trained in a visually grounded learning paradigm learns to
% represent linguistic information in a hierarchy, encoding form in the
% lower and meaning in the higher layers.  \dieuwke{This is of course
%   not very extensive, but maybe this is not really the place to put a
%   too elaborate analysis?}

% \paragraph{Behavioral studies}
% An entirely different line of research concerning understanding how RNNs process different types of phenomena by looking at their behavior when presented with carefully selected inputs.
% A large part of these studies has focusses on using the perplexity assigned to different sentences by a neural language model to investigate a range of (psycho)linguistic phenomena, such as subject-verb agreement \cite{Linzen:etal:2016,Bernardy:Lappin:2017,Gulordava:etal:2018,Kuncoro:etal:2018a,Kuncoro:etal:2018b,Linzen:Leonard:2018}, negative polarity items \cite{marvin2018targeted,jumelet2018language} and filler-gap dependencies \cite{wilcox2018rnn}.
% Their results -- LSTM language models are capable of correctly processing a number of such interesting linguistic phenomena -- are the premise of our study, in which we investigate \textit{how} they do so.

% \paragraph{Neuroscientific studies}
% \dieuwke{Should we include a section like this to refer to Nelson but potentially also other studies that present a more neurosciency take on this?}
% - Relate to Nelson et. al 2017 PNAS, an intracranial study that identifies electrodes whose high-gamma activity correlates with syntactic tree-depth (numnber of open nodes)
